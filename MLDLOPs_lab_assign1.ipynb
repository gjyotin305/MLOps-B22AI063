{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QayBcY_2VU8o",
        "outputId": "df728324-7fd7-4fa3-cd61-3c4866689548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.46)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.51.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2026.1.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login 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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh4ZYlN2VZW6",
        "outputId": "c3420c2b-bed2-401b-bbbc-8a93f4010a5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c086b3ed",
        "outputId": "8ebe7e6d-b87d-4b7b-eff9-76a12c80c023"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Define data transformations\n",
        "# Normalization parameters for CIFAR-10\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# 2. Load the CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "val_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_val\n",
        ")\n",
        "\n",
        "# 3. Create PyTorch DataLoaders\n",
        "batch_size = 128\n",
        "num_workers = 2 # Using 2 workers for demonstration, adjust based on system capabilities\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # No need to shuffle validation data\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "print(\"Data preparation complete. Training and validation DataLoaders created.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete. Training and validation DataLoaders created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a614eedf",
        "outputId": "f220999f-4f66-40b2-8e7f-1a69f43da5f2"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Define the CNN Model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # Convolutional Block 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Output: 32x16x16\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # Output: 64x8x8\n",
        "\n",
        "        # Convolutional Block 3\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # Output: 128x4x4\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        # The output of the last conv layer is 128 channels * 4 * 4 spatial dimensions\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 10) # 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv Block 1\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        # Conv Block 2\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        # Conv Block 3\n",
        "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        # Flatten the tensor for the fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 2. Instantiate the model\n",
        "model = CNNModel()\n",
        "\n",
        "# 3. Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"CNNModel instantiated and moved to {device}.\")\n",
        "print(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNModel instantiated and moved to cuda.\n",
            "CNNModel(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e4ec924",
        "outputId": "f267069b-0543-4f3a-9fb0-142cebad0ed3"
      },
      "source": [
        "!pip install thop"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from thop) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->thop) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->thop) (3.0.3)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "912334ca",
        "outputId": "32987fb0-8b5e-46d3-8d7c-19bbe91d85f9"
      },
      "source": [
        "from thop import profile\n",
        "\n",
        "# Create a dummy input tensor (batch_size=1, 3 channels, 32x32 image for CIFAR-10)\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "\n",
        "# Calculate FLOPs and parameters\n",
        "macs, params = profile(model, inputs=(dummy_input, ))\n",
        "\n",
        "# Convert FLOPs to GFLOPs\n",
        "gflops = macs / 1e9\n",
        "\n",
        "print(f\"Model FLOPs: {gflops:.2f} GFLOPs\")\n",
        "print(f\"Model parameters: {params/1e6:.2f} M\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "Model FLOPs: 0.01 GFLOPs\n",
            "Model parameters: 1.15 M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "6fa8150d",
        "outputId": "a9b82431-b5db-4aa7-81ab-d7f48c77e43b"
      },
      "source": [
        "import wandb\n",
        "\n",
        "# 1. Define hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 10,  # Placeholder, will be adjusted during actual training\n",
        "    \"batch_size\": batch_size, # Reusing batch_size from earlier setup\n",
        "    \"optimizer\": \"Adam\", # Common choice\n",
        "    \"loss_function\": \"CrossEntropyLoss\", # Common choice for multi-class classification\n",
        "    \"model_architecture\": \"CNNModel\",\n",
        "    \"dataset\": \"CIFAR-10\",\n",
        "    \"num_workers\": num_workers # Reusing num_workers from earlier setup\n",
        "}\n",
        "\n",
        "# 2. Initialize a Weights & Biases run\n",
        "# Replace 'your_username_or_team' with your actual W&B entity if applicable\n",
        "wandb.init(\n",
        "    project=\"CIFAR10_CNN_Training\",\n",
        "    entity=None, # Set to your W&B entity name if you have one, otherwise keep None\n",
        "    config=hyperparameters\n",
        ")\n",
        "\n",
        "print(\"WandB run initialized with the following hyperparameters:\")\n",
        "for key, value in hyperparameters.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">neat-moon-1</strong> at: <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/lnct1715' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/lnct1715</a><br> View project at: <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260201_131758-lnct1715/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260201_131812-q2qhux8t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/q2qhux8t' target=\"_blank\">soft-plant-2</a></strong> to <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/q2qhux8t' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/q2qhux8t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WandB run initialized with the following hyperparameters:\n",
            "  learning_rate: 0.001\n",
            "  epochs: 10\n",
            "  batch_size: 128\n",
            "  optimizer: Adam\n",
            "  loss_function: CrossEntropyLoss\n",
            "  model_architecture: CNNModel\n",
            "  dataset: CIFAR-10\n",
            "  num_workers: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "88c862b2",
        "outputId": "266dd304-ee94-4489-e171-f7f71b8c070e"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Set the number of epochs in the hyperparameters dictionary\n",
        "hyperparameters[\"epochs\"] = 25\n",
        "\n",
        "# 2. Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "# 4. Move the model to the specified device (already done in previous step, but ensuring here)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Training will run for {hyperparameters['epochs']} epochs.\")\n",
        "print(f\"Loss Function: {hyperparameters['loss_function']}\")\n",
        "print(f\"Optimizer: {hyperparameters['optimizer']}\")\n",
        "\n",
        "# 5. Implement the main training loop\n",
        "for epoch in range(hyperparameters[\"epochs\"]):\n",
        "    # Training Phase\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    # Log training loss and accuracy to WandB\n",
        "    wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy}, step=epoch)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations during validation\n",
        "        for data in val_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    # Log validation loss and accuracy to WandB with commit=True\n",
        "    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy}, step=epoch, commit=True)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{hyperparameters['epochs']} - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "# End the WandB run\n",
        "wandb.finish()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training will run for 25 epochs.\n",
            "Loss Function: CrossEntropyLoss\n",
            "Optimizer: Adam\n",
            "Epoch 1/25 - Train Loss: 1.5809, Train Acc: 42.18%, Val Loss: 1.2059, Val Acc: 55.49%\n",
            "Epoch 2/25 - Train Loss: 1.2319, Train Acc: 55.51%, Val Loss: 1.0690, Val Acc: 62.07%\n",
            "Epoch 3/25 - Train Loss: 1.0879, Train Acc: 61.17%, Val Loss: 0.9131, Val Acc: 67.97%\n",
            "Epoch 4/25 - Train Loss: 1.0051, Train Acc: 64.42%, Val Loss: 0.8294, Val Acc: 70.77%\n",
            "Epoch 5/25 - Train Loss: 0.9486, Train Acc: 66.61%, Val Loss: 0.7724, Val Acc: 72.54%\n",
            "Epoch 6/25 - Train Loss: 0.9020, Train Acc: 68.43%, Val Loss: 0.8067, Val Acc: 71.18%\n",
            "Epoch 7/25 - Train Loss: 0.8683, Train Acc: 69.63%, Val Loss: 0.7325, Val Acc: 74.58%\n",
            "Epoch 8/25 - Train Loss: 0.8324, Train Acc: 71.29%, Val Loss: 0.7433, Val Acc: 74.17%\n",
            "Epoch 9/25 - Train Loss: 0.8065, Train Acc: 71.85%, Val Loss: 0.7029, Val Acc: 75.36%\n",
            "Epoch 10/25 - Train Loss: 0.7750, Train Acc: 73.10%, Val Loss: 0.6796, Val Acc: 76.00%\n",
            "Epoch 11/25 - Train Loss: 0.7562, Train Acc: 73.68%, Val Loss: 0.6341, Val Acc: 77.92%\n",
            "Epoch 12/25 - Train Loss: 0.7237, Train Acc: 75.03%, Val Loss: 0.6376, Val Acc: 78.12%\n",
            "Epoch 13/25 - Train Loss: 0.7091, Train Acc: 75.48%, Val Loss: 0.6409, Val Acc: 78.01%\n",
            "Epoch 14/25 - Train Loss: 0.6933, Train Acc: 76.16%, Val Loss: 0.5887, Val Acc: 79.67%\n",
            "Epoch 15/25 - Train Loss: 0.6764, Train Acc: 76.49%, Val Loss: 0.6052, Val Acc: 79.40%\n",
            "Epoch 16/25 - Train Loss: 0.6614, Train Acc: 77.19%, Val Loss: 0.6341, Val Acc: 77.76%\n",
            "Epoch 17/25 - Train Loss: 0.6472, Train Acc: 77.63%, Val Loss: 0.6002, Val Acc: 79.26%\n",
            "Epoch 18/25 - Train Loss: 0.6346, Train Acc: 78.31%, Val Loss: 0.6081, Val Acc: 79.23%\n",
            "Epoch 19/25 - Train Loss: 0.6323, Train Acc: 78.32%, Val Loss: 0.5508, Val Acc: 80.72%\n",
            "Epoch 20/25 - Train Loss: 0.6118, Train Acc: 78.98%, Val Loss: 0.5521, Val Acc: 80.89%\n",
            "Epoch 21/25 - Train Loss: 0.6009, Train Acc: 79.53%, Val Loss: 0.5463, Val Acc: 80.66%\n",
            "Epoch 22/25 - Train Loss: 0.5925, Train Acc: 79.62%, Val Loss: 0.5286, Val Acc: 81.92%\n",
            "Epoch 23/25 - Train Loss: 0.5796, Train Acc: 80.15%, Val Loss: 0.5967, Val Acc: 80.03%\n",
            "Epoch 24/25 - Train Loss: 0.5671, Train Acc: 80.59%, Val Loss: 0.5274, Val Acc: 81.78%\n",
            "Epoch 25/25 - Train Loss: 0.5628, Train Acc: 80.76%, Val Loss: 0.5239, Val Acc: 82.15%\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇████▇██</td></tr><tr><td>val_loss</td><td>█▇▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>80.76</td></tr><tr><td>train_loss</td><td>0.5628</td></tr><tr><td>val_accuracy</td><td>82.15</td></tr><tr><td>val_loss</td><td>0.52388</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">soft-plant-2</strong> at: <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/q2qhux8t' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/q2qhux8t</a><br> View project at: <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260201_131812-q2qhux8t/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3e46cf61",
        "outputId": "fb16a9db-b43c-4ce9-fd0d-4912988e272f"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "\n",
        "# 1. Re-initialize a Weights & Biases run\n",
        "# Re-using hyperparameters from previous setup\n",
        "wandb.init(\n",
        "    project=\"CIFAR10_CNN_Training\",\n",
        "    entity=None, # Set to your W&B entity name if you have one, otherwise keep None\n",
        "    config=hyperparameters\n",
        ")\n",
        "print(\"WandB run re-initialized.\")\n",
        "\n",
        "# 2. Set the number of epochs in the hyperparameters dictionary\n",
        "hyperparameters[\"epochs\"] = 25\n",
        "\n",
        "# 3. Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "# 5. Move the model to the specified device (already done in previous step, but ensuring here)\n",
        "model.to(device)\n",
        "\n",
        "# 6. Call wandb.watch() to log gradients and weights\n",
        "#    This should be called after the model has been moved to the device and wandb.init()\n",
        "wandb.watch(model, log='all')\n",
        "\n",
        "print(f\"Training will run for {hyperparameters['epochs']} epochs.\")\n",
        "print(f\"Loss Function: {hyperparameters['loss_function']}\")\n",
        "print(f\"Optimizer: {hyperparameters['optimizer']}\")\n",
        "print(\"WandB is now watching model gradients and weights.\")\n",
        "\n",
        "# 7. Implement the main training loop\n",
        "for epoch in range(hyperparameters[\"epochs\"]):\n",
        "    # Training Phase\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    # Log training loss and accuracy to WandB\n",
        "    wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy}, step=epoch)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations during validation\n",
        "        for data in val_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    # Log validation loss and accuracy to WandB with commit=True\n",
        "    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy}, step=epoch, commit=True)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{hyperparameters['epochs']} - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "# End the WandB run\n",
        "wandb.finish()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260201_132755-n1pyz388</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/n1pyz388' target=\"_blank\">breezy-moon-3</a></strong> to <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/n1pyz388' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/n1pyz388</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WandB run re-initialized.\n",
            "Training will run for 25 epochs.\n",
            "Loss Function: CrossEntropyLoss\n",
            "Optimizer: Adam\n",
            "WandB is now watching model gradients and weights.\n",
            "Epoch 1/25 - Train Loss: 0.5561, Train Acc: 80.92%, Val Loss: 0.5132, Val Acc: 82.16%\n",
            "Epoch 2/25 - Train Loss: 0.5504, Train Acc: 81.26%, Val Loss: 0.5165, Val Acc: 82.42%\n",
            "Epoch 3/25 - Train Loss: 0.5368, Train Acc: 81.62%, Val Loss: 0.5511, Val Acc: 81.98%\n",
            "Epoch 4/25 - Train Loss: 0.5338, Train Acc: 81.74%, Val Loss: 0.5186, Val Acc: 82.33%\n",
            "Epoch 5/25 - Train Loss: 0.5298, Train Acc: 81.92%, Val Loss: 0.5710, Val Acc: 80.74%\n",
            "Epoch 6/25 - Train Loss: 0.5203, Train Acc: 82.51%, Val Loss: 0.4867, Val Acc: 83.34%\n",
            "Epoch 7/25 - Train Loss: 0.5094, Train Acc: 82.56%, Val Loss: 0.5180, Val Acc: 82.13%\n",
            "Epoch 8/25 - Train Loss: 0.5060, Train Acc: 82.57%, Val Loss: 0.5070, Val Acc: 82.32%\n",
            "Epoch 9/25 - Train Loss: 0.5027, Train Acc: 82.76%, Val Loss: 0.4940, Val Acc: 83.29%\n",
            "Epoch 10/25 - Train Loss: 0.4918, Train Acc: 83.06%, Val Loss: 0.4927, Val Acc: 83.19%\n",
            "Epoch 11/25 - Train Loss: 0.4892, Train Acc: 83.34%, Val Loss: 0.4822, Val Acc: 83.00%\n",
            "Epoch 12/25 - Train Loss: 0.4844, Train Acc: 83.53%, Val Loss: 0.4786, Val Acc: 83.49%\n",
            "Epoch 13/25 - Train Loss: 0.4764, Train Acc: 83.69%, Val Loss: 0.4821, Val Acc: 83.84%\n",
            "Epoch 14/25 - Train Loss: 0.4729, Train Acc: 83.79%, Val Loss: 0.5202, Val Acc: 82.55%\n",
            "Epoch 15/25 - Train Loss: 0.4730, Train Acc: 83.74%, Val Loss: 0.4719, Val Acc: 84.16%\n",
            "Epoch 16/25 - Train Loss: 0.4644, Train Acc: 84.00%, Val Loss: 0.4795, Val Acc: 84.15%\n",
            "Epoch 17/25 - Train Loss: 0.4630, Train Acc: 84.11%, Val Loss: 0.4799, Val Acc: 83.95%\n",
            "Epoch 18/25 - Train Loss: 0.4559, Train Acc: 84.33%, Val Loss: 0.4863, Val Acc: 83.53%\n",
            "Epoch 19/25 - Train Loss: 0.4565, Train Acc: 84.38%, Val Loss: 0.4971, Val Acc: 83.19%\n",
            "Epoch 20/25 - Train Loss: 0.4482, Train Acc: 84.73%, Val Loss: 0.4686, Val Acc: 84.13%\n",
            "Epoch 21/25 - Train Loss: 0.4451, Train Acc: 84.59%, Val Loss: 0.4685, Val Acc: 83.67%\n",
            "Epoch 22/25 - Train Loss: 0.4441, Train Acc: 84.85%, Val Loss: 0.4785, Val Acc: 84.18%\n",
            "Epoch 23/25 - Train Loss: 0.4379, Train Acc: 84.99%, Val Loss: 0.5189, Val Acc: 82.81%\n",
            "Epoch 24/25 - Train Loss: 0.4347, Train Acc: 85.07%, Val Loss: 0.4792, Val Acc: 83.83%\n",
            "Epoch 25/25 - Train Loss: 0.4299, Train Acc: 85.28%, Val Loss: 0.4867, Val Acc: 84.16%\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▂▂▂▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▇▇▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▄▄▄▄▁▆▄▄▆▆▆▇▇▅███▇▆█▇█▅▇█</td></tr><tr><td>val_loss</td><td>▄▄▇▄█▂▄▄▃▃▂▂▂▅▁▂▂▂▃▁▁▂▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>85.276</td></tr><tr><td>train_loss</td><td>0.42985</td></tr><tr><td>val_accuracy</td><td>84.16</td></tr><tr><td>val_loss</td><td>0.4867</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">breezy-moon-3</strong> at: <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/n1pyz388' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training/runs/n1pyz388</a><br> View project at: <a href='https://wandb.ai/gjyotin/CIFAR10_CNN_Training' target=\"_blank\">https://wandb.ai/gjyotin/CIFAR10_CNN_Training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260201_132755-n1pyz388/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}